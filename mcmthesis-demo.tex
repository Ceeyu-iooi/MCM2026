%% This is file `mcmthesis-demo.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% mcmthesis.dtx  (with options: `demo')
%% 
%% -----------------------------------
%% This is a generated file.
%% 
%% Copyright (C) 2010 -- 2015 by latexstudio
%%       2014 -- 2019 by Liam Huang
%%       2019 -- present by latexstudio.net
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%% 
%% The Current Maintainer of this work is latexstudio.net.
%% 
%% !Mode:: "TeX:UTF-8"
\documentclass{mcmthesis}
 %\documentclass[CTeX = true]{mcmthesis}  % 当使用 CTeX 套装时请注释上一行使用该行的设置
\mcmsetup{tstyle = \color{red}\bfseries,  % 修改题号，队号的颜色和加粗显示，黑色可以修改为 black
          tcn = 2615954, problem = C,     % 修改队号，参赛题号
          sheet = true, titleinsheet = true, keywordsinsheet = true,
          titlepage = false, abstract = true}

  %四款字体可以选择
  %\usepackage{times}
  %\usepackage{newtxtext}
  %\usepackage{palatino}
\usepackage{txfonts}

\usepackage{caption}  
\captionsetup[figure]{font=small}   % 将图标题字号设为small
\captionsetup[table]{font=small}    % 将图标题字号设为small

\usepackage{indentfirst}  % 首行缩进，注释掉，首行就不再缩进
\usepackage{esint}        % 积分号
\usepackage{subcaption}   % 并排图片
\usepackage{floatrow}     % 并排图片

\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

\usepackage{xcolor}       % 颜色支持
\usepackage{tabularray}   % 增强表格功能
\usepackage{lastpage}     % 获取最后一页页码

\usepackage{appendix}

% 核心修改：把复数 Appendices 改成单数 Appendix
\renewcommand{\appendixtocname}{Appendix}
\renewcommand{\appendixpagename}{Appendix}

\usepackage[style=numeric,backend=biber]{biblatex}
%\usepackage[style=apa,backend=biber]{biblatex}  % APA风格: style=apa
\ExecuteBibliographyOptions{sorting=none}           % 按引用顺序
\addbibresource{references.bib}                     % bib参考文献可用kimi生成

% 修复fancyhdr警告
\setlength{\headheight}{14pt}
\addtolength{\topmargin}{-1.6pt}  % 可选：保持总版心不变

% \setcounter{tocdepth}{2}  % 设置目录到二级标题, 将目录控制在1页
                          % 若可接受目录2页, 注释掉该句, 将到三级标题
                          
\title{\bf \Large Analysis and Optimization of DWTS Voting Fairness -Uncertainty Propagation and KMDR Fusion Mechanism }%

% \author{\small \href{https://www.latexstudio.net/}
%   {\includegraphics[width=7cm]{mcmthesis-logo}}}

\date{\today} 


%正文部分

\begin{document}

\begin{abstract}
\vspace {-2pt} 
\par Throughout the 34-season history of Dancing with the Stars (DWTS), significant discrepancies between professional judges' scores and audience voting preferences have sparked multiple controversial eliminations, exposing core flaws in the existing elimination mechanism's balance between fairness and entertainment value. Based on 2,777 historical records, we employed multi-model statistical methods to systematically investigate the rationality of the voting mechanism and proposes optimization solutions.



% In the renowned entertainment competition show Dancing with the Stars (DWTS), significant discrepancies exist between professional judges' scores and audience voting preferences. This has led to multiple controversial eliminations throughout the show's history, exposing fairness flaws in the existing elimination mechanism. Based on 2,777 historical records from 34 seasons, this paper employs multi-model statistical methods to systematically investigate the rationality of the voting mechanism and proposes optimization solutions.






\vspace {2pt} 

{\bf For Problem 1(Estimating Hidden Audience Votes)}, we constructed a dual-reconstruction modeling framework combining {\bf TAN Bayesian Networks}  and {\bf Random Forest} + {\bf SHAP} to reverse-engineer audience vote data while quantifying uncertainty. The TAN model achieved 82.48\% accuracy, the RF model reached 84.90\%, and the two models demonstrated 88.76\% consistency in predictions, enabling robust estimation through cross-validation.

\vspace {2pt} 
{\bf For Problem 2(Voting Synthesis Mechanism Comparison)}, we established a {\bf Variance Decomposition Model} based on the voting data from Problem 1 to quantify the differing impacts of the percentage method and ranking method on elimination outcomes. Results reveal that the {\bf Percentage Method} amplifies audience voting influence to 85.6\%, compared to 49.8\% under the Rank Method,indicating a pronounced entertainment-oriented skew. The Judges' Save mechanism successfully corrects 8.9\% of controversial eliminations , with synergistic effects when combined with Rank Method significantly enhancing procedural fairness.

\vspace {2pt} 
{\bf For Problem 3(Feature Attribution Quantification)} , we established a {\bf Mixed-Effects Model} , calculating ICC to decompose variance contributions from dancers and celebrity characteristics. Analysis shows that 57.73\% of performance variance is attributable to dancer skill differentials, while 48.50\% of judge score variance stems from dancer effects . DAG causal inference reveals that celebrity fan base indirectly influences final rankings through audience voting, with age and weekly performance scores exhibiting higher direct impact weights.

\vspace {2pt} 
{\bf For Problem 4(Mechanism Optimization)},  we proposed a {\bf KMDR Dynamic Weighting Model} based on Borda-Condorcet fusion, balancing professionalism and entertainment through distortion penalties $\alpha$  and consensus rewards $\beta$ . Parameter grid search  identified the optimal configuration ($\alpha$=0.4, $\beta$=0.6). Simulation results show: 100\% undisputed champion rate, 96.3\% early elimination rationality, and  82.7\% constraint rate for controversial contestants.

% We also constructed an uncertainty propagation framework to analyze the transmission of uncertainty from voting data estimation through mechanism rationality analysis to the quantification of eigenvalue contributions. Finally, we conducted robustness and parameter sensitivity analyses on the model while evaluating its strengths and weaknesses.
\vspace {3pt} 
We further developed an {\bf Uncertainty Propagation Framework} to achieve uncertainty quantification across the entire process:  voting estimation $\to$ mechanism evaluation $\to$ feature contribution analysis. We also conducted sensitivity analysis to validate the model's robustness to parameter perturbations and examined its strengths and weaknesses. Finally, we drafted a memorandum for the program team summarizing our research findings and providing recommendations.



\begin{keywords}
TAN,Random Forest,Mixed-Effects,Voting Mechanism,KMDR 
\end{keywords}

\end{abstract}

%目录部分
\maketitle

%% Generate the Table of Contents, if it's needed.

\begingroup
% 调整目录的行距系数，0.9 表示比正常缩小10%，可根据需要微调
\renewcommand{\baselinestretch}{0.8}
\tableofcontents
\endgroup


\newpage

\section{Introduction}
\subsection{Problem Background}
“Dancing with the Stars (DWTS)” is a globally influential entertainment competition program originating from the UK's “Strictly Come Dancing.” Having successfully run for 34 seasons, its spin-off versions now span multiple regions worldwide. The core format pairs celebrities with professional dancers, combining professional judges' scores with audience votes to determine eliminations and final rankings.

However, during its long-term operation, a core dilemma has gradually emerged: Despite consistently receiving low scores from professional judges for their dance performances and lacking recognition for their technical proficiency, some celebrity contestants advance through the competition—and even win—by leveraging high audience support. This stark divergence between the judges' professional assessments and the public's preferences has not only sparked widespread controversy but also directly exposed the inherent flaws in the existing voting system. \cite{toupiao}Consequently, optimizing the voting and elimination mechanisms has become an urgent issue requiring resolution for the show.


\subsection{Restatement of the Problem}

Considering the background information and restricted conditions identified in the problem
statement, we need to solve the following problems:

\begin{itemize}
\item {\bf Problem 1:} Develop a model to estimate hidden audience voting data and quantify its uncertainty.
\item {\bf Problem 2:} Compare the rationality of two voting synthesis mechanisms (Ranking and Percentage) in DWTS, and quantify the impact of the judge rescue mechanism.
\item {\bf Problem 3:}  Quantify the influence of contestant characteristics, judge scores, and other factors on elimination outcomes and voting results, identifying core influencing factors.
\item {\bf Problem4:} Design an optimal voting mechanism that balances multiple objectives including program fairness and entertainment value.
\end{itemize}



\subsection{Our work}

In order to clearly illustrate our work, we draw the flowchart Figure \ref{fig1}.

\begin{figure}[H]    % H表示强制固定在当前位置
\small
\centering
\includegraphics[width=1.0\textwidth]{workflow_diagram_v2.drawio1.png}
\caption{Our work} \label{fig1}
\end{figure}

\section{Preparation for Models}  

Considering those practical problems always contain many complex factors, first of all,
we need to make reasonable assumptions to simplify the model, and each hypothesis is closely
followed by its corresponding explanation:


\subsection{Assumptions and Notations}

\begin{itemize}
\item {\bf Assumption 1:} The publicly available data provided  is authentic and valid, with missing values and outliers accounting for an extremely low proportion that does not affect the overall statistical patterns.
ble to assume the high quality of their data.
\item {\bf Assumption 2:} Audience voting behavior exhibits stability, and there exists a significant statistical correlation between audience votes and core variables such as judges' scores and contestant characteristics.

\item {\bf Assumption 3:} The judges' scoring is professional and objective, with results accurately reflecting contestants' dance proficiency. Scoring discrepancies follow a normal distribution.


\item {\bf Assumption 4:} Uncertainty arises solely from estimation errors in hidden voting data, while errors in model specification, parameter calibration, and other related processes are negligible.
\end{itemize}



The key mathematical notations used in this paper are listed in Table \ref{tbl1}. 

%% 三线表
\begin{table}[H]     % H表示强制固定在当前位置
\small
\centering           % 设置居中
\caption{Notations}  % 表标题
\label{tbl1}         % 设置表的引用标签
\begin{tabular}{cl}  % 2列, c表示居中对齐, l和r表示左右对齐
\toprule             % 画顶端横线
{\bf Symbol}    &  {\bf Definition}     \\
\midrule             % 画中间横线
$V_i$     &Estimated  Fans Vote Score of the $i$-th contestant  \\
$Y$     & $Y \in \{0, 1\}:$Elimination Flag (0=Not Eliminated, 1=Eliminated)  \\
$X$   & $X = (X_1, X_2, ..., X_n):$Feature Vetor \\
$S_i$  & Judge score of the $i$-th contestant \\
$R_i$  & The rank of the $i$-th contestant   \\
$C_i$  & Final score for contestant $i$  \\
$W_{fan}$ or $W_{judge}$  & Fan vote weight or judge score weight   \\
\bottomrule         % 画底部横线
\end{tabular}
\vspace{2pt} % 稍微调整间距
\begin{flushleft} \small
$^{*}$ There are some variables that are not listed here and will be discussed in detail in each
section.
\end{flushleft}
\end{table}

\subsection{Data Processing}

Preliminary exploratory analysis of the dataset reveals three core issues: imputed missing values, numerical anomalies, and inconsistent feature formats. Systematic preprocessing is required to eliminate data quality risks and provide reliable support for subsequent modeling and analysis. Accordingly, following the MS08 data processing specifications and solution workflow, comprehensive preprocessing is conducted focusing on missing value and outlier handling, as well as data transformation.

\begin{figure}[H]    % H表示强制固定在当前位置
\small
\centering
\includegraphics[width=1.0\textwidth]{shuju.png}
\caption{Data Processing Workflow} \label{fig_dp}
\end{figure}

As shown in the {\bf Figure \ref{fig_dp}}, differentiated processing strategies are implemented for three scenarios involving latent missing values. During the feature transformation phase, standardized processing is applied to different feature types. Feature importance calibration is completed based on SHAP value analysis, identifying elimination risk coefficient, cumulative ranking, and relative performance as core influencing features. Their SHAP values are 0.32, 0.28, and 0.21 respectively, demonstrating high alignment with the actual impact mechanisms of the competition. This provides a high-quality data foundation and parameter support for subsequent modeling analysis.






\section{Fan Vote Estimation Based on TAN and SHAP Models }
Regarding the controversy surrounding celebrity contestants receiving low scores from judges yet achieving high final rankings in the program, to understand this discrepancy between judge scores and audience support, we establish a model to reverse-engineer these undisclosed audience voting data while quantifying the uncertainty of such estimates.\cite{inverse}

\subsection{Tree Augmented Naive Bayes Model for Fan Vote Estimation}
In the program dataset, certain features exhibit dependency. For example:$week\_rank$ is highly correlated with $cumulative\_rank$ (partial correlation coefficient = 0.553);$remaining\_weeks$ is strongly negatively correlated with $weeks\_participated$ (partial correlation coefficient = -0.665)
Ignoring these dependencies degrades the performance of traditional Bayesian models. 
{\bf TAN (Tree Augmented Naive Bayes)} is an extension of Naive Bayes \cite{beiyesi} that allows for tree-like dependency structures between features:

\begin{equation}
P(Y \mid X) \propto P(Y)\cdot P(X_{root} \mid Y)\cdot \prod_{j\ne root} P(X_j \mid Pa(X_j), Y)
\end{equation}



\begin{figure}[H]    % H表示强制固定在当前位置
% \caption{Maximum Spanning Tree of Feature Dependency} \label{fig_model1}

% \begin{subfigure}{0.4\textwidth}
% \includegraphics[width=1.0\textwidth]{fig1-1.png}
% \caption{TAN Network Structure} \label{fig1-1}
%  \end{subfigure}
%  \begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.5\textwidth]{fig1-2.png}
\caption{Conditional Mutual Information Matrix}
\label{fig_1-2}
% \end{subfigure}\\




\end{figure}


To determine the optimal feature dependency structure, we first compute the conditional mutual information between all feature pairs:
\begin{equation}
\begin{aligned}
I(X_i; X_j | Y) = \sum_y \sum_{x_i} \sum_{x_j} P(x_i, x_j, y) \cdot \log\left[ \frac{P(x_i, x_j | y)\cdot P(x_j | y)} {P(x_i | y)} \right]
\end{aligned}
\end{equation}





As shown in the {\bf Figure \ref{fig_1-2}}, conditional mutual information measures the statistical dependence between features $X_i$ and $X_j$ given the category $Y$. A higher value indicates that both features must be considered together when predicting $Y$.
Using conditional mutual information as edge weights, construct a maximum spanning tree using {\bf Prim's algorithm}.






For a new sample $X = (X_1, X_2, ..., X_n)$, compute the posterior probability via Softmax normalization:
\begin{equation}
\begin{aligned}
P(Y = 1 \mid X) &= \frac{\exp(\ell_1)}{\exp(\ell_0) + \exp(\ell_1)}, \\
\ell_j &= \log \tilde{P}_j \quad (j=0,1)
\end{aligned}
\end{equation}

Convert elimination probability to audience vote score (0--100 points):

\begin{equation}
\begin{aligned}
V &= (1 - P( (Y=1) \mid X)) \times 100
\end{aligned}
\end{equation}



\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=1.0\textwidth]{fig1-5.png}
\caption{Fan Vote Score Distribution} \label{fig_1-5}
\end{figure}
As shown in the {\bf Figure \ref{fig_1-5}}, this represents the distribution of audience votes.A lower elimination probability indicates higher audience support.
After solving the TAN model,as shown in the {\bf Figure \ref{fig_TAN_result}}, the model achieved an {\bf Accuracy Rate} of { $\bf 82.48\%$}  and a {\bf ROC-AUC Value} of {\bf 0.8102}, indicating that the TAN model can effectively distinguish eliminated contestants from those who advanced.
%  The recall for the elimination category was $56.14\%$, indicating the model identified over half of actual eliminations; precision was $46.72\%$, with an F1 score of $51.00\%$. Collectively, these metrics reflect the model's classification capability on the imbalanced dataset (where eliminations constituted only $16.24\%$ of events).

% By modeling conditional dependencies between features, the TAN model learned nine significant feature dependency edges in the {\bf Figure \ref{fig_1-4}}
%  (e.g., $week\_rank\rightarrow cumulative\_rank$, 
% $weeks\_participated\rightarrow remaining\_weeks$). 
% These dependencies align with the actual logic of dance competitions—cumulative rankings are highly correlated with weekly rankings, while the number of weeks participated naturally correlates negatively with the number of weeks remaining.


\begin{figure}[H] 
\caption{Confusion Matrix and ROC Curve} 
\centering
\label{fig_TAN_result}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=0.9\textwidth]{fig1-3.png}
\caption{Confusion Matrix(TAN Model)} \label{fig_1-3}
\end{subfigure}\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.8\textwidth]{fig1-4.png}
\caption{ROC Curve(Area Visualization)} \label{fig_1-4}
\end{subfigure}\\
\end{figure}

% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=0.43\textwidth]{fig1-6.png}
% \caption{Feature Importance}\label{fig_1-6}
% \end{figure}




\subsection{RF and SHAP Model for Fan Vote Estimation}

To cross-validate with the TAN model, we developed a combined approach integrating random forests with SHAP explainability analysis to infer audience voting scores from elimination results. The Random Forest prediction formula is:
\begin{equation}
\begin{aligned}
\hat{y} = \text{mode} \{h_{1} (x), h_{2} (x), \ldots, h_{k} (x)\}
\end{aligned}
\end{equation}
Here, $h_{k}(x)$ denotes the prediction result of the $k-$th decision tree, and $\text{mode}$ represents majority voting.
For binary classification problems, the elimination probability is calculated as:
\begin{equation}
\begin{aligned}
P\left( (Y=1) \mid x\right) = (1/K) \times \sum_{k} I\left(h_{k}\left(x\right) = 1\right)
\end{aligned}
\end{equation}


where $K$ is the number of decision trees, and $I(\cdot)$ denotes the indicator function. Based on the elimination probability, the audience voting distribution can be derived using the formula.


% \begin{figure}[H]    % H表示强制固定在当前位置
% \begin{subfigure}[H]    % H表示强制固定在当前位置

% \includegraphics[width=0.4\textwidth]{fig3_rf_confusion_matrix.png}

% \caption{Confusion Matrix(Random Forest Model)} \label{fig3_rf_confusion_matrix}
% \end{subfigure}\begin{subfigure}[H]    % H表示强制固定在当前位置

% \includegraphics[width=0.4\textwidth]{fig4_rf_roc_curve.png}

% \caption{ROC Curve(Random Forest Model)} \label{fig4_rf_roc_curve}
% \end{subfigure}\\

% % \includegraphics[width=0.4\textwidth]{fig4_rf_roc_curve.png}

% % \caption{ROC Curve(Random Forest Model)} \label{fig4_rf_roc_curve}
% \end{figure}

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.5\textwidth]{fig5_rf_fan_vote_distribution.png}

\caption{Fan Vote Score Distribution(Random Forest Model)} \label{fig5_rf_fan_vote_distribution}
\end{figure}

\begin{figure}[H] 
\caption{RF Model Confusion Matrix and ROC Curve} 
\centering
\label{fig_rf_result}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{fig3_rf_confusion_matrix.png}
\caption{Confusion Matrix} \label{fig3_rf_confusion_matrix}
\end{subfigure}\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{fig4_rf_roc_curve.png}
\caption{ROC Curve} \label{fig4_rf_roc_curve}
\end{subfigure}\\
\end{figure}











As shown in the {\bf figure \ref{fig_rf_result}}, the model achieves an {\bf accuracy rate} of $\bf 84.90\%$ and a {\bf ROC-AUC value} of {\bf 0.8860}, outperforming the TAN model across all evaluation metrics. Notably, the recall rate for the elimination category has increased to $76.32\%$, with an F1 score reaching $61.27\%$, indicating that the Random Forest model demonstrates significantly stronger capability in identifying elimination events.

SHAP assigns a “contribution score” to each feature based on the Shapley value from game theory, indicating the feature's impact on the model's prediction.
The Shapley value is calculated using the following formula:
\begin{equation}
 \begin{aligned}
\phi_{i} &= \sum_{S\subseteq N\backslash\{i\}} [\frac{|S|!(|N|-|S|-1)!}{|N|!}] \times [f(S\cup\{i\}) - f(S)]
\end{aligned} 
\end{equation}
Here, $N$ denotes the feature set, $S$ represents the subset excluding feature $i$, and $f$ is the model's prediction function. Global feature importance is defined as the average of the absolute values of SHAP values across all samples.{\bf Figure \ref{fig2_rf_shap_summary}}  presents a feature importance summary chart, which explains the impact of each feature on the prediction results within the “elimination prediction” model.


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.5\textwidth]{fig2_rf_shap_summary.png}

\caption{SHAP Summary Plot(Random Forest Model)} \label{fig2_rf_shap_summary}
\end{figure}


\subsection{Consistency Test of Two Models}
By ranking the importance of feature quantities for the two models and conducting consistency analysis, as shown in {\bf Table \ref{tbl2}}, we obtain the following results:

% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=0.9\textwidth]{fig1_rf_feature_importance.png}

% \caption{Feature Importance Ranking(Random Forest Model)} \label{fig1_rf_feature_importance}
% \end{figure}




% 三线表
\begin{table}[H]     % H表示强制固定在当前位置
\small
\centering           % 设置居中
\caption{Consistency Metrics for the Two Models}  % 表标题
\label{tbl2}         % 设置表的引用标签
\begin{tabular}{llp{0.5\textwidth}}  % 2列, c表示居中对齐, l和r表示左右对齐
\toprule             % 画顶端横线
\textbf{Consistency Metrics}          & \textbf{Value} & \textbf{Statistical Interpretation} \\  
\midrule             % 画中间横线
Spearman Correlation Coefficient      & 0.8029         & High positive correlation between two models' predictions (p\textless{}0.001) \\
Pearson Correlation Coefficient       & 0.8359         & Extremely strong linear correlation                                           \\
Prediction Direction Consistency Rate & 88.76\%        & Nearly 90\% of samples show consistent prediction directions                  \\
Mean Absolute Error (MAE)             & 13.35         & Prediction differences fall within an acceptable range                        \\
Feature Ranking Correlation           & 0.9650         & Feature importance rankings are nearly identical \\
\bottomrule         % 画底部横线
\end{tabular}
\end{table}


The two models exhibit a highly positive correlation in predicting audience vote scores (Spearman $\rho$ ={\bf 0.8029}, p<0.001,as showed in {\bf Figure \ref{fig6_rf_tan_comparison}}), with an $\bf 88.76\%$ agreement rate in prediction direction. This indicates that despite employing fundamentally different methodologies—TAN using generative probabilistic modeling and RF using discriminative ensemble learning—both models yield highly consistent predictions on identical data.

Furthermore, as shown in {\bf Figure \ref{fig5_rf_fan_vote_distribution} }, both models predicted audience voting scores exhibiting a reasonable bimodal distribution: advancing contestants averaged around 80 points (TAN: 79.42, RF: 81.31), while eliminated contestants averaged approximately 25--30 points. The approximately 55-point difference between the two groups perfectly aligns with the competition's elimination logic (low-scoring contestants are eliminated). The statistical characteristics of the prediction results align with the elimination patterns in the original data, validating the models' validity.



\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.5\textwidth]{fig6_rf_tan_comparison.png}

\caption{Model Agreement: Random Forest vs TAN} \label{fig6_rf_tan_comparison}
\end{figure}






%% 定理类环境
% \begin{Theorem} \label{thm1}
% Let $A$ ...
% \end{Theorem}
% 
% \begin{Lemma} \label{lem1}
% ...
% \end{Lemma}
% 
% \begin{proof}
% The proof of theorem.
% \end{proof}


% %% 并排子图
% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \begin{subfigure}{0.53\textwidth}
% \centering
% \includegraphics[height=5.5cm]{fig3a.png}
% \caption{Risk levels in different regions in U.S.} 
% %\label{fig3a}
% \end{subfigure}
% \begin{subfigure}{0.4\textwidth}
% \centering
% \includegraphics[height=5.5cm]{fig3b.png}
% \caption{Risk level} 
% %\label{fig3b}
% \end{subfigure}
% \caption{Risk rating and map presentation} \label{fig3}
% \end{figure}





% \begin{equation}
% \frac{E(r_i)-r_f}{\beta_i} = E(r_m)-r_f
% \end{equation}

% \begin{equation}
% \beta_i = \frac{\mathrm{Cov}(R_i,R_m)}{\mathrm{Var}(R_m)} = \rho(i,m) \frac{\sigma_i}{\sigma_m}
% \end{equation}

% \noindent where

% % \begin{itemize}
% % \item $E(r_{i})$ is the expected return on the capital asset,
% % \item $E(r_{m})$ is the expected return of the market,
% % \item $r_{f}$ is the risk-free rate of interest such as interest arising from government bonds,
% % \item $\beta_{i}$ is the sensitivity of the expected excess asset returns to the expected excess market returns,
% % \item $\rho(i,m)$ denotes the correlation coefficient between the investment $i$ and the market $m$,
% % \item $\sigma_{i}$ is the standard deviation for the investment $i$,
% % \item $\sigma_m$ is the standard deviation for the investment $m$.
% % \end{itemize}

% Expected return on investment (ROI):

% \begin{equation}
% ROI = \int \frac{-A \times l(p)+(1+x)A \times R_{Value}-I}{I}f(p) \mathrm{d}p
% \end{equation}

% Variance of ROI:

% \begin{equation}
% \mathrm{Var}(ROI) = \int \bigg[\frac{-A \times l(p)+A(1+x) \times R_{value}-l-(Ax \times 
% R_{value}-l)}{I}\bigg]^2 f(p) \mathrm{d}p
% \end{equation}

% \noindent where

% \begin{itemize}
% \item $A$ is the sum insured,
% \item $I$ is the invested capital,
% \item $l(p)$ is the loss function,
% \item $x$ is the surcharge rate.
% \end{itemize}

% The surcharge rate $x$ is a multiple of the average value of the loss:

% \begin{equation}
% x = \frac{\Big(1+r_f+\rho \times \frac{(A/I) \times \sigma}{\sigma_M} \times 
% (r_M-r_f) \Big) \times I}{A \times R_{value}} \label{eq14}
% \end{equation}

% where $\sigma$ is the Standard deviation of $R_{value}$, $y$ is Pure premium per $\$10,000$:

% \begin{equation}
% y = 10000(1+x) \times P
% \end{equation}

% \noindent where $P$ is the probability of a disaster causing damage.

% Typically, it is more reasonable to spend $3-10$ percent of each person's annual income on
% insurance.We assume that each person is willing to spend $5\%$ of his or her annual income each
% year to purchase catastrophe insurance with a one-year term. Insurance companies can make
% decisions from two perspectives based on the above formula:

% \begin{itemize}
% \item Introducing bankruptcy theory, after calculating the lowest order price, y, in the case
% where the probability of the firm's future bankruptcy is less than $10\%$, and then comparing
% it to the local per capita annual disposable income (GNI), it is expected that
% people in the locality will not be able to afford to consume catastrophe insurance and
% will not invest in it if the ratio of premiums per $10,000$ to GNI is greater than $5\%$.

% \item We use $5\%$ of the local national GNI per capita as the subscription price per $\$10,000$
% of premium. If this price makes the likelihood of future insolvency of the company
% higher than $10\%$, no investment is made in that location.
% \end{itemize}

% The price of insurance also affects people's desire to buy to some extent, and an increase
% in the price of insurance may lead to a decrease in their desire to buy.

% \begin{equation}
% N = (1-\omega y) N_A
% \end{equation}

% \begin{equation}
% Total~Revenue = y \times N
% \end{equation}

% \noindent where

% \begin{itemize}
% \item $N_A$ is the total local population,
% \item $N$ is the number of local people with a strong desire to buy,
% \item $\omega$ is the factor that influences the price of insurance on the willingness of locals to buy, and is related to the average disposable income of locals as well as the gap between the rich and the poor,
% \item $Total~Revenue$ is the projected total local insurance revenue.
% \end{itemize}

% The company first determines the area in which it wants to invest money to build the
% insurance and then determines the price of local insurance. We would like to maximize the
% company's total revenue:

% \begin{equation}
% \begin{aligned}
% & \max ~~ Total~Revenue \\
% & \text{s.t.} ~~ 
% \begin{cases}                % cases环境生成左大括号
% 0 < \omega y < 1 \\
% y \geqslant y_{10} \\
% y < 0.05 \times GNI
% \end{cases}
% \end{aligned}
% \end{equation}

% where $y_{10}$ is the price of insurance when the firm's insolvency rate is $10$ percent.

% Our model is implemented in Gorontalo, Indonesia and Los Angeles, California. This is
% because both locations have similar and high risk indices, with Los Angeles having the highest
% disaster risk index in the United States.

% %% 并排放两个子图
% \begin{figure}[H]      % H表示强制固定在当前位置
% \small
% \centering
% \begin{subfigure}{0.5\textwidth}
% \centering
% \includegraphics[height=5.5cm]{fig4a.jpg}
% \caption{Gorontalo}    % \label{fig4a}
% \end{subfigure}
% \begin{subfigure}{0.45\textwidth}
% \centering
% \includegraphics[height=5.5cm]{fig4b.jpg}
% \caption{Los Angeles}   % \label{fig4b}
% \end{subfigure}
% \caption{Location of the two areas on the map} \label{fig4}
% \end{figure}

% After searching for relevant data, we calculated that in order to ensure that the probability
% of the company's bankruptcy after investing in catastrophe insurance in Gorontalo is less than
% $10\%$, we need to charge a premium of $\$342.745$ for every $\$10,000$ of coverage, which is calculated
% in equation \eqref{eq14}, of which $\$283.465$ is the pure premium and $\$59.28$ is the additional
% premium. Searching for relevant information we find that $5\%$ of the per capita disposable income (GNI) of 
% Gorontalo is only $\$137.25$, so the likelihood of residents being willing to purchase
% catastrophe insurance is low and the company should not invest in catastrophe insurance
% in the area.

% In order to ensure that the probability of insolvency of the company after investing in
% catastrophe insurance in Los Angeles is less than $10\%$, through the formula \eqref{eq14} 
% calculated that for every $\$10,000$ of coverage need to charge a premium of $\$295.09$ of 
% which the pure premium is $\$200$ and the additional premium is $\$95.09$ (because of the higher 
% return on investment in the market in the U.S.). The per capita disposable income in 
% Los Angeles ($5\%$ of GNI is $\$3,162.65$), which is much higher than the cost of catastrophe 
% insurance. In order to determine the most appropriate cost of insurance to earn a greater benefit, 
% we plotted the trend of total premium income as a function of premiums.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \includegraphics[width=0.5\textwidth]{fig5.jpg}
% \caption{Relationship between company revenue and insurance price in Los Angeles} \label{fig5}
% \end{figure}

% From the Figure \ref{fig5}, it can be seen that with the increase of premiums, the total income of
% insurance companies tends to increase first and then decrease. This is because when the premium
% is too low, although the number of insured people is high, the amount of single transaction
% is small and the number of guarantees is too high, which leads to a higher risk of bankruptcy
% of the insurance company; whereas too high a premium will reduce the consumer's expectations
% of catastrophe insurance, and the volume of insurance orders will be small.

% In summary, for Los Angeles, a premium of about $2,500$ per $\$10,000$ of coverage can be
% used, and $1.92$ million people are expected to purchase the company's catastrophe insurance
% (the total population of Los Angeles is about $3.79$ million). At this point, the insurance company's
% theoretical revenue would be around $\$4.5$ billion. Although Los Angeles has a high risk
% index, the profits are equally attractive, so the insurance company could take the risk of launching
% its catastrophe insurance business here.





% Catastrophe bonds are risk-linked securities that transfer a specified set of risks from a
% sponsor to investors. Catastrophe bonds emerged from a need by insurance companies to alleviate
% some of the risks they would face if a major catastrophe occurred, which would incur
% damages that they could not cover by the invested premiums \cite{jaffee1997, froot1999}. An insurance company issues
% bonds through an investment bank, which are then sold to investors. These bonds are inherently
% risky, and usually have maturities less than 3 years. If no catastrophe occurred, the insurance
% company would pay a coupon to the investors. But if a catastrophe did occur, then the principal
% would be forgiven and the insurance company would use this money to pay their claim-holders.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \includegraphics[width=0.65\textwidth]{fig6.jpg}
% \caption{Insurance securitization schema} \label{fig6}
% \end{figure}

% From an economic perspective, the securitization of insurance, particularly through instruments
% like catastrophe bonds, represents a significant innovation in the capital markets.
% This innovation not only diversifies investment opportunities but also plays a crucial role in
% enhancing the resilience of the insurance industry against catastrophic events. Catastrophe
% bonds allow insurance companies to transfer the risk of extreme events, such as natural disasters,
% to the capital markets, thereby reducing their potential liability and improving their solvency.
% This mechanism enables insurance firms to manage their risk exposure more effectively
% and to maintain stability in the face of potentially ruinous events. By doing so, it also ensures
% that insurance companies can continue to offer coverage for risks that might otherwise be uninsurable
% due to their catastrophic potential.



% The government plays an important role in the country. The government can make some
% appealing policies to stimulate people to buy insurance and cooperate with insurance companies
% to undertake part of the risk. When people buy insurance, individuals are only required to bear part of the premium. The remainder is subsidized by the various levels of government. If
% necessary, special groups of people may be fully covered by government finances \cite{michel2011}. When a
% catastrophe occurs, the government can act as a reinsurer and bear part of the amount of compensation.
% If the amount of compensation is small, the insurance company will pay directly.
% Otherwise, it can be covered or partially paid by the government. In this way, a multi-layered
% diversification of risk is constructed. It not only brings benefit protection to the people, but also
% drives the development of the insurance industry \cite{aase1992}.



% Through the two scenarios described above, the insurance company's market return on
% investment in the Gorontalo region $r_m$ increased. When a natural disaster occurs, the amount
% of compensation paid by the insurance company is shared by the insurance company, the investors
% in the insurance securities, and the local government. In addition, the government subsidizes
% residents for catastrophe insurance, which increases the willingness of residents to purchase
% catastrophe insurance and reduces the actual cost paid by individuals. The insurance
% company can set premiums at the lowest premium ($\$295.09$) that can be assumed under the
% risk of insolvency. We plot the trend of total premium income as a function of premium at this
% point in time.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \includegraphics[width=0.5\textwidth]{fig7.jpg}
% \caption{The Analog trend of total premium income in Gorontalo} \label{fig7}
% \end{figure}

% As we can see from the picture. Under the company's affordable insolvency risk, the insurer
% expects maximum revenues of $\$245$ million. The company expects maximum revenue is
% $\$245$ million. At this time ,each $\$10,000$ of insurance amount charges $\$295.09$ of insurance
% premiums. We expect $830,255$ people (about $73.26\%$ of the total population) to have catastrophic
% insurance. Gorontalo residents pay only $\$137.25$ individually, and the remainder is
% subsidized by the Gorontalo government. The government guarantees the legal rights of Gorontalo
% residents as well as their social welfare.


% Our insurance model has a significant impact on the development decisions of real estate developers. Based on the above model derivation, it can be learned that for areas with high
% natural disasters and low per capita income, if the insurance company is willing to underwrite
% policies, it will result in the high bankruptcy rate of the company not being able to realize
% profitability \cite{blanchard1994}. Similarly, real estate developers will not choose the area for investment and development
% due to high risk and lack of demand. That is, any area with high R value and income
% (GNI) below a certain value is not recommended for real estate developers to invest in. In
% addition to this, such areas have the following risk factors: 

% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \includegraphics[width=0.5\textwidth]{fig8.jpg}
% \caption{Risk factors} \label{fig8}
% \end{figure}

% Further applying our model, we can calculate the insurance rate, which is the insurance
% premium divided by insurance amount. If the area has a high insurance rate by calculating,
% the property developer would have to bear a higher insurance cost during the construction of
% the building as well as during the unsold period. Therefore, property developers need to carefully
% consider and weigh the future profit and loss before making decisions.

% Similarly, in other areas, we can calculate local insurance rates based on our model. According
% to this indicator, property developer can further determine the cost of developing land
% in local area and buying insurance. In this way we provide a reference for the property developer’s
% decision making.

% Additionally, our model can also provide guidance about how property developers build
% construction. For each of the $18$ hazard types, we can calculate the value of EAL and Rvalue (in
% dollars) for each hazard type. We find a positive correlation between EAL and Rvalue to some
% extent. Thus, property developers can determine the different major hazrard type for each area
% based on Rvalue and thus build different types of homes. For example, in the city of New Orleans,
% USA, flooding ranks high on the list of $18$ natural disasters in terms of Rvalue. Accordingly,
% many property companies, such as American Restorators LLC, are building houses with high
% foundations locally to minimize damage and achieve business profitability. Our model solves
% the problem about how to build on certain site. This approach not only maintains the interests
% of real property developers, but also protects the lives of people in the community.



\section{Voting Combinations Comparison via Variance Decomposition Model}

Following our discussion of Problem 1, we have now obtained an effective and accurate estimate of audience voting. Next, we will study the voting aggregation mechanism based on this voting estimate.There are two voting combinations in the program:
\begin{itemize}
\item Rank Method:Each contestant receives a judge ranking based on the judges' scores and an audience ranking based on viewer votes. The two rankings are combined to form a composite ranking, and the contestant with the highest composite ranking is eliminated.

\begin{equation}
\begin{aligned}
C_{Rank,i} = R_{judge,i}+R_{fan, i}
\end{aligned}
\end{equation}
\item Percentage Method:Each contestant's judge scores and audience votes are converted into percentages of the total score, then combined to form a composite percentage. The contestant with the lowest composite percentage is eliminated.
\begin{equation}
\begin{aligned}
 C_{Pct,i}= \frac{S_i}{\sum_j S_j}+\frac{V_i}{\sum_j V_j}
\end{aligned}
\end{equation}
\end{itemize}




\subsection{Counterfactual Simulation Modeling}


To present the simulation results of both methods, we employed counterfactual simulation modeling to apply both approaches to all 34 seasons, constructing a comparative experiment.Load all 34 seasons' judge scores and predicted audience vote data to simulate the ranking method: for each elimination week, compute judge rankings and audience rankings to derive a composite ranking; simulate the percentage method: for each elimination week, compute judge percentages and audience percentages to derive a composite percentage.

Compare the eliminated contestants predicted by each method against the actual eliminated contestants, and calculate the proportion of identical decisions produced by both methods.As shown in the {\bf Figure~\ref{fig_counterfactual}}, both methods yielded identical results in 91.1\% of elimination decisions. This indicates that the impact of method selection is relatively limited, as judges' and viewers' preferences align in most cases. The percentage method achieved a slightly higher prediction accuracy (34.30\%) than the ranking method (29.26\%). This may be attributed to the fact that Seasons 3-27 actually employed the percentage method, allowing viewers' voting behavior to adapt to this approach.

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.9\textwidth]{q2_fig1_counterfactual.png}

\caption{Counterfactual Simulation Modeling} \label{fig_counterfactual}
\end{figure}

\subsection{Bias Sensitivity Analysis}
To quantify the impact of audience voting on the final outcome, we employed bias sensitivity analysis,Decomposing the variance of the composite score into judge components and audience components, and calculating the contribution of the audience component to the total variance (audience weighting formula):
\begin{equation}
\begin{aligned}
W_{fan} =\frac{Var(Fan\_Component)}{Var(Combined\_Score)}
\end{aligned}
\end{equation}
To measure the impact of minor shifts in audience votes on elimination probability, Marginal Effect analysis is employed:
\begin{equation}
\begin{aligned}
Marginal\_Effect = \frac{\partial P(Y=1)}{\partial V_i}
\end{aligned}
\end{equation}
% Simultaneously, calculate the gradient of the composite score with respect to changes in audience votes to measure sensitivity.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=1.0\textwidth]{q2_fig2_bias_sensitivity.png}

% \caption{Bias Sensitivity Analysis} \label{fig_bias_sensitivity}
% \end{figure}


\begin{figure}[H] 
\caption{Bias Sensitivity Analysis} 
\centering
\label{fig_bias_sensitivity}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{q2_fig2_bias_sensitivity1.png}
\end{subfigure}\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{q2_fig2_bias_sensitivity2.png}
\end{subfigure}\\
\end{figure}


As shown in the {\bf Figure \ref{fig_bias_sensitivity}}, the weight of audience votes in the {\bf percentage method} {\bf (85.6\%)} is {\bf significantly higher} than that in the ranking method {\bf (49.8\%)}, with a difference of 35.8 percentage points. The mathematical reason for this discrepancy is that in the ranking method, regardless of how many audience votes a contestant receives, their contribution is identical as long as their ranking remains the same. In contrast, the percentage method directly reflects the absolute difference in vote counts through the percentage.  This implies that during the percentage method era (Seasons 3-27), contestants with strong fan bases—such as Bristol Palin and Bobby Bones were more likely to secure a significant audience vote advantage, thereby offsetting lower judge scores.




\subsection{Analysis of Controversial Cases and Method Evaluation}

Based on the above discussion, we analyze cases where contestants such as Bristol Palin, Bobby Bones, Jerry Rice, and Billy Ray Cyrus received extremely low judge scores yet still achieved high rankings during the season. For each controversial contestant, we simulate their performance under two voting methods: calculating how many times they would be eliminated under the ranking system versus the percentage system. If a contestant faces more eliminations under one method, it indicates that method is more ``disadvantageous'' to them.



\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{q2_fig3_controversy_cases.png}

\caption{Analysis of Controversial Cases} \label{fig_controversy_cases}
\end{figure}


As shown in the {\bf Figure \ref{fig_controversy_cases}}, Bristol's case demonstrates the percentage method's bias toward contestants with large fan bases. Her audience vote percentile (73.6\%) significantly exceeded her judge vote percentile (60.2\%), and this disparity was fully leveraged under the percentage system to propel her into the finals. The Bobby Bones case exemplifies the extreme bias of the percentage method. As a radio host, he commands a massive fanbase, which translates into a substantial audience vote advantage under this system. This advantage completely offsets his lower dance technique scores.



\subsection{Discussion on the Judge Rescue Mechanism}

Starting from Season 28, DWTS introduced the “judges' save” mechanism. Once the bottom two contestants are determined, the judges may vote to save one of them. This provides an opportunity for professional judgment to correct “unfair” eliminations. By utilizing judges' scores from all seasons, elimination results, and fan vote scores predicted by the random forest model in Problem 1, we can calculate the prediction accuracy of elimination using the three methods shown in the {\bf Figure \ref{fig_judge_save}}, thereby {\bf quantifying} the impact of the judges' save mechanism.


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=1.0\textwidth]{q2.png}

\caption{Analysis of the Impact of Judges' Save Mechanism} \label{fig_judge_save}
\end{figure}

The judge's lifeline mechanism altered approximately 8.9\% of elimination outcomes. Although its prediction accuracy was relatively low (25.4\%), it provided an opportunity to correct errors in disputed cases and increased the weight of professional judgment.

We evaluated the two methods across three dimensions: prediction accuracy, weight balance, and Arrow's theorem scores.

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.6\textwidth]{q2_fig5_final_analysis.png}

\caption{Multi-Criteria Analysis of two Methods} \label{fig_final_analysis}
\end{figure}

Therefore, the primary recommendation is the {\bf Ranking System combined with the Judges' Save Mechanism}. This approach has been adopted by DWTS since Season 28. Within the framework of Arrow's theorem, the ranking system achieves a higher fairness score, while the judges' save mechanism provides an opportunity to correct approximately 9\% of controversial eliminations.



% Building value is measured in terms of the building's cultural value and community influence,
% economic value, and historical value. Therefore, we take these three main aspects as primary
% indicators.

% \subsubsection{Indicators Determination}

% For the cultural value and community influence, we synthesized various factors, such as
% geography and network, and finally selected the three most representative secondary indicators
% to construct our model. Similarly, for the economic and historical value, we selected two secondary
% indicators each to improve the model. The specific description and indicators selected
% are shown in Table \ref{tbl3}.

% % 创建Excel表格(可合并单元格), 导入在线生成表格: https://www.latex-tables.com/
% % 设置网格线、右侧可设置背景色

% \begin{table}[H]
% \small
% \centering
% \caption{Indicators}  
% \label{tbl3}         
% \begin{tblr}{
%   row{odd} = {LightGreen},
%   row{1} = {ForestGreen},
%   cell{2}{1} = {r=3}{LightGreen},
%   cell{5}{1} = {r=2}{},
%   cell{7}{1} = {r=2}{},
%   vlines,
%   hline{1-2,5,7,9} = {-}{},
%   hline{3-4,6,8} = {2-3}{}}
% Object    & Indicators & Description      \\
% {Cultural Values and \\ Community influence} 
%    & NG  & Number of Google search terms   \\
%    & P   & {Participation in events held \\ around the building} \\
%    & ANV & Annual number of visitors        \\
% Economy  & LV         & Land value         \\
%          & CC         & Construction cost  \\
% History  & NH         & Number of historical research documents  \\
%          & DP         & Degree of preservation
% \end{tblr}
% \end{table}


% \begin{itemize}
% \item Cultural Value and Community Influence
% \begin{description}
% \item [$\circ$] Global Visibility

% The cultural value of a building depends to a large extent on its global visibility. So we
% quantify its global visibility through two metrics, ``Number of Google search terms''
% (NG) and ``Annual number of visitors'' (ANV). This approach balances online and offline,
% making the measurement of cultural values more quantifiable and accurate.

% \item [$\circ$] Impact on the Community

% Buildings have a strong connection with local communities. When measuring the value
% of a building, we take into account its impact on the local community. Research has
% shown that the more influence a building has on the local community, the more the
% value of the landmark itself will increase. Besides, it will further promote the increase
% of influence, realizing a positive feedback loop. Therefore, we choose ``Participation in
% events held around the building'' (P) to quantify the building's influence on the community.
% We calculate P as follows:

% \begin{equation}
% P = \frac{1}{N} \times \sum_{i = 1}^N \frac{NCMP_i}{NTC_i}
% \end{equation}

% where $NCMP_i$ represents ``Number of community members participating in activities''
% at the $i$th activity, $NTC_i$ means the total number of people in the community at the
% time of the $i$th activity and N means the total number of activities conducted around
% the building.

% \end{description}

% \item Economy Value

% For economic value, we mainly consider the value of the building in terms of its construction.
% Therefore, we considered the value of the land it occupies. And it is measured
% by the indicator ``Land value'' (LV).

% \begin{equation}
% LV = P_c \times Area
% \end{equation}

% where $P_c$ represents the current price of the land and Area represents the area occupied by
% the building. Meanwhile, for the value created during the construction of the building itself,
% we use ``Construction cost'' (CC) for quantitative assessment. Taking inflation into account,
% we define Construction cost as all costs involved in the implementation of that construction
% project under this year's Engineering News-Record (ENR) benchmark for the region. Both
% the $LV$ and $CC$ metrics are expressed in U.S. dollars.

% \item Historic Value

% \begin{description}
% \item [$\circ$] Historical Research Value

% The historical value of a building is largely dependent on its place in historical research.
% So we quantify its visibility and importance in the academic world through NH. NH
% refers to the number of historical research documents related to the building, including
% but not limited to books, papers, reports, etc. This indicator reflects the building's attention
% and depth of research in the historical community. The higher NH value means
% the building has a higher historical research value.

% \item [$\circ$] The Preservation Condition

% The historic value of a building is also affected by its state of preservation. We use
% ``Degree of preservation'' (DP) to measure the extent to which a building has been preserved
% from its original state. It includes aspects such as structural integrity, exterior
% preservation, and interior decoration. The assessment of DP can be based on expert
% review, preservation grade, and comparative analysis with the original state. Highly
% preserved buildings not only better transmit history and culture, but also provide rich
% materials for future research.
% \end{description}
% \end{itemize}

% \subsubsection{Weight Calculation}

% CRITIC is an objective assignment method based on data volatility. The idea of this
% method was based on two indicators, contrast intensity and correlation indicators. When calculating
% the weights, we need to multiply the contrast intensity with the correlation indicator
% and then normalize to get the final weights.

% \begin{itemize}
% \item Contrast intensity refers to the magnitude of the difference in values between evaluation
% programs for the same indicator, expressed as a standard deviation. The larger the standard
% deviation, the greater the fluctuation . That is, the larger the difference in the values taken between
% the programs, the higher the weight will be.
% \item The Sperman correlation coefficient is used to express the correlation between indicators.
% If there is a strong positive correlation between two indicators, it means that the less conflicting
% they are, the lower the weight will be.
% \end{itemize}

% 1) There are $n$ samples to be evaluated and $p$ evaluation indicators to form the raw indicator
% data matrix.
% \[
% X = \begin{pmatrix}
% x_{11} & \cdots & x_{1p} \\
% \vdots & \ddots & \vdots \\
% x_{n1} & \cdots & x_{np}
% \end{pmatrix}
% \]

% where $x_{ij}$ represents the value of the $j$th evaluation indicator for the $i$th sample.

% 2) In order to remove the effect of the scale each indicator is normalized. The indicators
% we selected are of benefit attributes type, so the normalization formula:
% \[
% X_{ij} = \frac{x_{ij}-\min(x_j)}{\max(x_j)-\min(x_j)}
% \]
% \noindent where $X_{ij}$ is normalized to obtain a numerical matrix.

% 3) Then we calculate the contrast intensity of the indicator:
% \[
% \left\{
% \begin{aligned}
% & \bar{x_j} = \frac{1}{n}\sum_{i=1}^{n} x_{ij} \\
% & S_j = \sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar{x_j})^2}{n-1}}
% \end{aligned}
% \right.
% \]
% \noindent where $S_j$ represents the strength of comparison of the $j$th indicator.

% The larger the $S_j$, the greater the difference in values for that indicator. The more information
% the indicator reflects, the stronger the evaluation strength of the indicator itself and
% the more weight should be assigned to it.

% 4) Calculation of the conflicting nature of the indicators
% \[
% \left\{
% \begin{aligned}
% & d_i = \mathrm{rank}(x_{ij})- \mathrm{rank}(x_{ik}) \\
% & r_{jk} = 1-\frac{6 \sum d_i^2}{n (n^2-1)} \\
% & R_j = \sum_{k=1,k\neq j}^p (1-r_{jk})
% \end{aligned}
% \right.
% \]
% \noindent where $R_{jk}$ denotes the Sperman correlation coefficient between evaluation indicators $j$ and
% $k$. $R_j$ denotes the conflictual of the $j$th indicator.

% The Sperman correlation coefficient is used to express the correlation between indicators.
% The stronger the correlation between two indicators, the less they conflict, the more they reflect
% the same information, and the more repetitive the content of the evaluation is. To a certain
% extent, the evaluation strength of the indicator is weakened and the weight assigned to it should
% be reduced.

% 5) Calculation of the amount of information:
% \[
% C_j = S_j \times R_j
% \]

% 6) Based on the amount of information, we calculate the weights of each indicator defined
% $w_j$:
% \[
% w_j = \frac{C_j}{\sum_{j=1}^p C_j}
% \]

% 7) The score for each indicator is:
% \[
% Score_{object} = \sum_{j=1}^p w_j s_j
% \]
% \noindent where $object$ represents ``Cultural values and community influence'', ``Economy'', ``History'',
% $s_j$ denotes the value of the $j$th secondary indicator.

% Applying the CRITIC weighting method for each level 1 indicator separately, the objective
% weights for each level 2 indicator were obtained as shown in the following Table \ref{tbl1}.

% \subsubsection{Quantitative Results of Building Values}

% In order to assign weights to these three level 1 indicators to get the final building value,
% we use hierarchical analysis to construct a judgment matrix to get the weights of the three level
% 1 indicators:
% \[
% \theta = (0.4432, 0.3873, 0.1694)
% \]
% \noindent where the consistency ratio of the judgment matrix $= 0.017591$, and the consistency is acceptable.
% Ultimately, our building impact score is calculated as follows:

% \begin{equation}
% V_{score} = \sum_{i=1}^3 \theta_i Score_{object}
% \end{equation}

% % 创建Excel表格(可合并单元格), 导入在线生成表格: https://www.latex-tables.com/
% % 设置网格线、右侧可设置背景色

% \begin{table}[H]
% \small
% \centering
% \caption{The weight of indicators}
% \label{tbl4}
% \begin{tblr}{
%   cells = {LightGreen},
%   row{1} = {ForestGreen},
%   row{3} = {},
%   cell{2}{1} = {r=3}{},
%   cell{2}{2} = {r=3}{Wheat},
%   cell{5}{1} = {r=2}{},
%   cell{5}{2} = {r=2}{Wheat},
%   cell{5}{3} = {},
%   cell{5}{4} = {},
%   cell{7}{1} = {r=2}{},
%   cell{7}{2} = {r=2}{Wheat},
%   cell{7}{3} = {},
%   cell{7}{4} = {},
%   vlines,
%   hline{1-2,5,7,9} = {-}{},
%   hline{3-4,6,8} = {3-4}{}}
% Object  & Weight & Indicators & Weight \\
% {Cultural Values and \\ Community influence} 
%    & 0.4432 & NG  & 0.1698 \\
%    &        & P   & 0.4429 \\
%    &        & ANV & 0.3873 \\
% Economy  & 0.3873 & LV         & 0.4272 \\
%          &        & CC         & 0.5728 \\
% History  & 0.1694 & NH         & 0.6286 \\
%          &        & DP         & 0.3714 
% \end{tblr}
% \end{table}

% % 更多颜色https://ctan.math.washington.edu/tex-archive/macros/latex/contrib/xcolor/xcolor.pdf

% \subsection{Determination of protection measures}

% \subsubsection{Measure Score}

% In Model 1, we obtained a composite risk score $R_{score}$ of each region by analyzing $18$
% natural hazards. Next, in the above section, we quantified the value of the building to get the
% score $V_{score}$. By multiplying the risk score and the value score, $M_{score}$ is obtained, which is
% used to assess the conservation priority of the building and the extent and scale of conservation
% measures that need to be taken.

% \begin{equation}
% M_{score} = V_{score} \times R_{score}
% \end{equation}

% A higher $M_{score}$ indicates a higher value of the building, along with a higher risk of exposure
% to natural hazards. Therefore, more urgent and comprehensive protection measures are
% needed. Based on the statistical distribution of $M_{score}$, we set reasonable thresholds to recognize
% low, medium, and heigh grades, and the values of the specific thresholds need to be set
% based on expert recommendations and industry standards.

% \subsubsection{Score of Protection Measures}

% \begin{itemize}
% \item {\bf Low:} For low-grade $M_{score}$ buildings, basic conservation measures, such as routine
% maintenance and inspections and, where necessary, minor repairs, are undertaken. The risk or
% value of these buildings is low, so the measures taken are mainly preventive and low-cost.
% \item {\bf Medium:} For medium-grade $M_{score}$, moderate protection measures are implemented, including
% enhanced structural inspections, improved safety features, and disaster preparedness
% programs. These measures aim to increase the resistance and resilience of buildings and require
% moderate investment.
% \item {\bf High:} For high-grade $M_{score}$, implement comprehensive and high-intensity protection
% measures. This may include comprehensive structural reinforcement, installation of advanced
% security systems, and customized risk management plans in cooperation with external experts.
% Given the high risk or value of these buildings, the goal of the measures is to minimize potential
% losses, even if this means higher initial costs.
% \end{itemize}

% {\bf Note that:} The ``one-size-fits-all'' approach to disaster protection measures ignores the impact
% of regional differences, architectural characteristics and socio-economic factors, and can
% lead to poor protection and resource utilization. So specific protection measures still need to be
% derived from a thoughtful local analysis by natural disaster experts.

% \subsubsection{Mentoring for Community Leaders}

% Our model provides a quantitative and systematic framework for community leaders to
% help them determine the extent and priority of preservation measures based on a building's risk

% and value scores. The model makes the decision-making process more scientific and accurate
% by combining risk scores (which consider threats such as natural hazards) and value scores
% (which include historical, cultural, economic, and community importance). It promotes optimal
% allocation of resources and ensures that high-value or high-risk buildings are adequately protected,
% while also taking into account economic benefits. In addition, the model encourages
% community involvement and support, improves disaster response capacity, and supports sustainable
% community development. Through this approach, community leaders are able to make
% more informed decisions to protect and maintain important buildings in their neighborhoods,
% contributing to the overall well-being and development of the community.

\section{Quantifying Feature Contributions via Mixed-Effects Model}
To quantify the contribution of dancer characteristics and contestant characteristics to judges' and audience scores, we established a mixed-effects model based on fan vote estimates to achieve this quantification.
\subsection{Calculate Fixed and Random Effects}

A {\bf Mixed Effects Model} is a statistical model that incorporates both fixed effects and random effects, suitable for data with hierarchical structures or repeated measurements\cite{laird_random-effects_1982}. Its basic form is:
\begin{equation}
\begin{aligned}
y_{ij} = X_{ij} \cdot \beta + Z_{ij} \cdot b_j + \epsilon_{ij}
\end{aligned}
\end{equation}
Where: $y_{ij}$ is the performance of star $i$ by dancer $j$ (judges' scores or final ranking); $X_{ij}$ is the fixed effects design matrix (star characteristics) ; $\beta$ is the fixed effects coefficient vector; $Z_{ij}$ is the random effects design matrix (typically intercepts) ; $b_j \sim N(0, {\sigma^2}_b)$ is the random effects for dancer j •;$\epsilon _{ij} \sim N(0, {\sigma^2}_\epsilon )$ is the residual error.

First, estimate the random effects by grouping the data by dancer and calculating the within-group mean for each dancer as the random effect estimate:
\begin{equation}
    \begin{aligned}
b\ _j = \bar{{y}\ _j} - \bar{y}
\end{aligned}
\end{equation}

Then estimate the fixed effects, subtract the group effects from the original dependent variable, and use Ridge regression to fit the fixed effects.
\begin{equation}  
\begin{aligned}
\beta = (X'X + \alpha I ) ^ {-1} X' (y - Zb )
\end{aligned}
\end{equation}
Where $\alpha$ is the regularization parameter to prevent overfitting; $I$ is the identity matrix.Through model estimation, significant fixed effects (contributions of player characteristics to performance) can be obtained, as shown in {\bf Table \ref{table_mixed}}.Variables such as $week\_std$, $avg\_place\_std$, $age\_std$, $age\_std$, and $season\_std$ significantly alter the ranking coefficient, reflecting pronounced fixed effects including stage history, age, and week effects.

% 三线表
\begin{table}[H]     % H表示强制固定在当前位置
\small
\centering           % 设置居中
\caption{Feature Variable Contributions}  % 表标题
\label{table_mixed}         % 设置表的引用标签
\begin{tabular}{llp{0.1\textwidth}lp{0.1\textwidth}}  % 2列, c表示居中对齐, l和r表示左右对齐
\toprule             % 画顶端横线
\textbf{Variable}  & \textbf{Weighting Factor} & \textbf{p-value} & Ranking Factor & \textbf{p-value} \\
\midrule 
seasons\_std       & +0.1103                   & 0.0020           & +0.2461        & 0.0009           \\
avg\_place\_std    & -0.4336                   & 0.0000           & -0.4053        & 0.0000           \\
age\_std           & +0.5666                   & 0.0178           & +1.0791        & 0.0000           \\
followers\_std     & +0.2779                   & 0.0299           & +0.0853        & 0.4933           \\
season\_std        & +0.2498                   & 0.0000           & +0.4058        & 0.0000           \\
week\_std          & -0.9351                   & 0.0000           & -0.9804        & 0.0000           \\
exp\_x\_fans       & +0.1478                   & 0.0279           & -0.0703        & 0.2772           \\
champ\_x\_fans     & -0.3599                   & 0.0268           & -0.0733        & 0.6544           \\
season\_x\_fans    & -0.1318                   & 0.0099           & -0.0846        & 0.1258           \\
ind\_Entertainment & -0.4823                   & 0.0025           & +0.0924        & 0.5936     \\
\bottomrule         % 画底部横线
\end{tabular}
\end{table}

Decomposing the variance in $week\_rank$ and $placement$ for the model reveals the contribution of the dancer effect to the total variation. Calculate the intraclass correlation coefficient ($ICC$):

\begin{equation}
  \begin{aligned}
ICC = \frac{\sigma^2_b}{\sigma^2_b + \sigma^2_\varepsilon}
\end{aligned}
\end{equation}
Through model estimation, we can determine the contribution of the professional dancer effect to the final ranking: 57.73\%, and to the judges' scores: 48.50\%. In the ranking model, $ICC$ = 0.4777, and in the scoring model, $ICC$ = 0.4305. $ICC$ values close to 0.5 indicate that approximately half of the variation stems from dancer differences, demonstrating that professional dancers significantly impact celebrity contestants' performance scores.


\subsection{Model Optimization}
To enhance the model's generalization capability,we systematically optimized the base model by incorporating the nonlinear term $age\_squared$ to capture the nonlinear effects of age and incorporated temporal trends via $season\_std$ and $week\_std$ to enhance interpretability and predictive power.  The optimized model achieved a 37.3\% increase in variance for the judge scoring model and a 35.9\% increase in variance for the ranking model. This enables better quantification of how features like professional dancer experience, celebrity age, fan count, and season count contribute to contestant performance scores,as shown in  {\bf Figure \ref{fig_effects_analysis_v2}}.


% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=1.0\textwidth]{q3_fig6_validation_r2_comparison.png}

% \caption{Model Improvement Variance Test} \label{fig_validation_r2_comparison}
% \end{figure}


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{q3_fig7_effects_analysis_v2.png}

\caption{Enhanced Model Effects Analysis} \label{fig_effects_analysis_v2}
\end{figure}


\subsection{An Investigation into the Influence of Judges and Audience Voting}
% A directed acyclic graph (DAG) is a visualization tool used to represent causal relationships between variables.\cite{neuberg_causality_2003} DAG explicitly distinguish between direct effects, indirect effects, and confounding effects. Based on mixed-effects model coefficients and domain knowledge, they identify the following four key causal pathways:


To better visualize the causal relationships between dancers' and contestants' characteristics and their performance outcomes, we employ a directed acyclic graph (DAG)\cite{neuberg_causality_2003}.Based on mixed-effects model coefficients, we identified the following four key causal pathways:


\begin{itemize}
\vspace{-8pt}
\item Dancer's Historical Record $\to$ Judges' Scores $\to$ Final Ranking
\vspace{-8pt}
\item Social media followers $\to$ Fan voting $\to$ Final ranking
\vspace{-8pt}
\item Star's Age $\to$ Judges' Scores / Fan Votes $\to$ Final Ranking
    \vspace{-8pt}
\item $Season \times Fans \to$ Judge Scores / Fan Votes $\to$ Final Ranking
\end{itemize}
  \vspace{-8pt}
Through interaction analysis, the {\bf Figure \ref{fig_causal_dag}} reveals causal pathways linking professional dancer characteristics and celebrity characteristics to fan and judge votes, while distinguishing the direct impact of judges' scores from the indirect influence of fan voting.As shown in {\bf Figure \ref{fig_effect_decomposition}}, the direct effect of dancer characteristics (25\%) is the largest, indicating  {`` Who is your dance partner? '' }  is the primary factor determining scores. The indirect effect of social media (18\%) is also significant, validating the importance of the fan voting mechanism.

Therefore, social media follower counts, industry background, and interaction metrics primarily influence judges' scores rather than follower-driven rankings. Champion dancers' partnerships and the age effect exert stronger influence on fan voting. This indicates that judges evaluate technical proficiency, while fans respond more broadly to star appeal.

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.5\textwidth]{q3_fig_causal_dag.png}

\caption{Causal DAG} \label{fig_causal_dag}
\end{figure}

  \vspace{-8pt}
\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.7\textwidth]{q3_fig_effect_decomposition.png}

\caption{Effect Decomposition} \label{fig_effect_decomposition}
\end{figure}







% We select Tokyo Tower in Japan as the landmark for evaluation and analysis.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \small
% \centering
% \includegraphics[width=0.5\textwidth]{fig9.jpg}
% \caption{Location of Tokyo Tower} \label{fig9}
% \end{figure}

% \subsection{Insurance Pricing for Tokyo Tower}

% Designed by Japanese architect Tachu Naito, the Tokyo Tower cost $\$8.4$ million to build
% at the time and was constructed to send broadcast signals in Tokyo. Currently, Tokyo's GNI
% per capita is $\$36,964.96$, and the Tokyo capital market is currently functioning well. The risk
% score for Tokyo is calculated to be 86. According to the insurance pricing model, the optimal
% insurance rate for an insurance company to issue catastrophe insurance in Tokyo is $3.8\%$. In
% view of the special historical value of the Tokyo Tower, the insurance company may appropriately
% increase the insurance rate. As a result, the Tokyo government spends approximately US
% $\$319,200$ per year on catastrophe insurance for the Tokyo Tower.

% \subsection{Architectural Value of Tokyo Tower}

% In 2011, a major earthquake struck northeastern Japan, and this earthquake caused some
% damage to Tokyo Tower, bending the antenna at the tip of the tower by $2$ degrees, resulting in
% the end of the experimental broadcasting of terrestrial wave digital sound broadcasting, and the
% interruption of the transmission of the $24/7$ terrestrial analog television signals. 
% Tokyo Tower entered a maintenance period during which analog signals and FM broadcasting-related services
% also began to be transferred to Tokyo Skytree, so the proportion of Tokyo Tower's actual
% use gradually decreased. However, Tokyo Tower has attracted more than $3$ million visitors as
% a tourist attraction and has accumulated more than $150$ million visitors. It is also a symbol of
% Japan's post-war prosperity and has a remarkable historical significance for the Japanese people.

% According to the calculations of the building conservation model, the Mscore of Tokyo
% Tower is located in a high conservation level area. Therefore, the government should consider
% purchasing catastrophe insurance for Tokyo Tower and strengthening its daily supervision and
% maintenance to ensure that it can withstand natural disasters, such as earthquakes, and that its
% seismic treatment measures also need to be strengthened.


\section{Insight:Uncertainty Propagation Framework}


The fan vote estimates for Question 1 are likely based on reverse-engineering from limited information, inherently carrying uncertainty. When these estimates serve as inputs for Question 2 and Question 3, how does this uncertainty affect the reliability of subsequent conclusions?

We establish a propagation framework designed to quantify and trace how uncertainty in Question~1 fan vote estimates propagates to Question~2 controversy identification and Question~3 factor analysis\cite{saltelli_sensitivity_2000}. As shown in \textbf{Figure~\ref{fig_propagation_framework}}, the confidence intervals for Question~1 fan vote estimates are quantified using Bootstrap resampling. Monte Carlo simulations are employed to propagate Question~1 uncertainty into the Question~2 controversy identification model. To analyze the impact of Question~1 uncertainty on Question~3 mixed-effects model coefficient estimates, a coefficient \textbf{Sensitivity Analysis} is conducted.


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{fig_propagation_framework (1).png}

\caption{Uncertainty Propagation Framework} \label{fig_propagation_framework}
\end{figure}


\textbf{Figure~\ref{fig_propagation_analysis}} was obtained through calculation. This matrix quantifies the uncertainty propagation amplification effect (Amplification Factor) between different questions (Question~1, Question~2, Question~3). Despite moderate uncertainty in Question~1 voting estimates (CI width = 0.336), conclusions from Question~2 dispute identification and Question~3 factor analysis remain stable. Only a slight amplification occurs along the Question~1~$\to$~Question~2 path (Uncertainty Amplification Factor of $1.13\times$), and Question~3's characteristic statistics remain significant after propagation. Thus, this model framework demonstrates strong robustness and overall reliability.


\begin{figure}[H] 
\caption{Uncertainty Propagation Analysis Results} 
\label{fig_propagation_analysis}\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{fig_uncertainty_propagation (1).png}
\end{subfigure}\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\textwidth]{fig_sensitivity_analysis (1).png}
\end{subfigure}\\
\end{figure}


\section{KMDR Model for A Score Fusion Mechanism }

\subsection{Establishment KMDR Model}

In reality competition shows like DWTS, the design of the scoring mechanism directly impacts the fairness and credibility of the competition. To address the potential systemic discrepancies between judges' scores and audience votes, we developed a KMDR (Kemeny-Metric Distortion Rank) model based on cutting-edge research in social choice theory and voting system design \cite{arrow_social_2012}.

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{q4.png}

\caption{KMDR Model Principle Workflow} \label{fig_kmdr_workflow}
\end{figure}

As shown in {\bf Figure~\ref{fig_kmdr_workflow}},Our goal in establishing the model is to design a composite scoring function $G = f(S, V, t)$ that satisfies the fairness, dynamic adaptability, accuracy, and interpretability.Below is the solution and performance evaluation of the scoring fusion mechanism.


\subsection{Performance Evaluation of Score Fusion Mechanism}


\begin{itemize}

\item Superiority of Simulation Results

Based on simulations using KMDR, the results are shown in the {\bf Figure~\ref{fig_champion_quality}}.  The figure indicates that the mean overall strength distribution of KMDR champions is 0.6502, with a standard deviation of 0.098, demonstrating that champions generally possess a high and stable overall level. Most crucially, none of the 32 KMDR champions were controversial selections—a stark contrast to the potential “one-sided dominance” issues present in existing systems. This demonstrates that KMDR does not merely “predict” existing outcomes but offers a fairer, more reasonable alternative.


% Across the simulated 32 seasons, we analyzed the quality of these champions from two dimensions: overall strength and controversy.

\begin{figure}[H] 
\caption{KMDR Simulation Analysis Results} 
\label{fig_propagation_analysis}\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{fig_simulation_champions.png}
\caption{KMDR Champion Quality Analysis} \label{fig_champion_quality}
\end{subfigure}\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{fig_simulation_elimination.png}
\caption{KMDR Early Elimination Rationality Analysis} \label{fig_elimination_rationality}
\end{subfigure}\\

\end{figure}


\vspace{-12pt}
\item Highly Rational Elimination Decision

As shown in the {\bf Figure \ref{fig_elimination_rationality}}, we analyzed the strength distribution of eliminated contestants over the first three weeks: 96.3\% of eliminated contestants ranked below the 20th percentile in overall strength, indicating highly rational elimination decisions. Unlike existing mechanisms that may prematurely eliminate contestants who are “weak with judges but strong with audiences” or “weak with audiences but strong with judges,” KMDR ensures that only contestants deemed weak by both parties are eliminated.


  \vspace{-8pt}


\item Significantly enhanced fairness

Beyond simulation evaluation, we also assess KMDR's fairness from a theoretical perspective. Based on social choice theory, we employ four independent metrics:
Satisfaction Balance, Noise Robustness,
Condorcet Efficiency ,
and Disagreement Penalty Correlation.


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.7\textwidth]{fig_fairness_metrics.png}

\caption{KMDR Fairness Metrics Analysis Results} \label{fig_fairness_metrics}
\end{figure}

As shown in the {\bf Figure \ref{fig_fairness_metrics}}, KMDR achieves the second-highest satisfaction balance after the percentage method, realizing mutual satisfaction among judges (0.958) and audiences (0.967). KMDR demonstrates a remarkable Condorcet efficiency of 98.79\%, ensuring that the “consensus favorite” receives the deserved ranking. Its disagreement penalty mechanism exhibits statistical significance (p<0.01), proving that the metric distortion theory's penalty mechanism effectively constrains “controversial contestants.”

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.6\textwidth]{fig_kmdr_weight_dynamics.png}

\caption{KMDR Dynamic Weight Adjustment Adaptation} \label{fig_kmdr_weight_dynamics}
\end{figure}


\vspace{-12pt}
\item Dynamic Weight Adjustment Adaptation

One of KMDR's core innovations is dynamically adjusting the judge-audience weighting based on the competition stage. {\bf Figure \ref{fig_kmdr_weight_dynamics}} illustrates how the weighting changes over the weeks of the competition (0.75 in the early stage, 0.60 in the middle stage, and 0.50 in the late stage). This aligns with the developmental patterns of reality TV shows: early screening requires professionalism, while the later finals demand public participation. This design balances both competitiveness and entertainment value.






\end{itemize}



\vspace{-10pt}

\section{Evaluation and Sensitivity Analysis}



\subsection{Validity Test and Robustness Analysis of RF Model}
Although the RF model in {\bf Problem 1} exhibits strong robustness against interference, it is prone to overfitting or underfitting issues, resulting in insufficient generalization capabilities. Therefore, we employ stratified 10-fold cross-validation to assess the model's generalization performance. Stratified sampling ensures that the proportion of positive and negative samples in each fold matches the original dataset, thereby preventing evaluation bias caused by class imbalance.


\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{fig_cv_boxplot.png}

\caption{Cross-validation Boxplot} \label{fig_cv_boxplot}
\end{figure}


As shown in {\bf Figure \ref{fig_cv_boxplot}}, the average accuracy of the random forest model with 10-fold cross-validation is 0.8524, with a standard deviation of only 0.0213. This indicates the model exhibits stable performance across different data partitions and possesses strong generalization capabilities. The ROC-AUC reaches 0.8906, demonstrating the model's excellent ability to distinguish classification boundaries. 
% Recall (0.6766) exceeds precision (0.5898), indicating the model tends to identify more rejected cases—aligning with the early warning system's design objectives.

% Learning curve analysis is used to assess whether a model is overfitting or underfitting. By observing the trend of training set and validation set accuracy across different training sample sizes, we evaluate the model's learning capability and generalization ability. As shown in {\bf Figure \ref{fig_learning_curve}},the gap between training set accuracy (0.9253) and validation set accuracy (0.8570) is 0.0682, indicating mild overfitting within an acceptable range. As the training sample size increases, the test set accuracy steadily improves, indicating that the model can effectively learn patterns within the data.

% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=0.6\textwidth]{fig_learning_curve.png}

% \caption{Learning Curve} \label{fig_learning_curve}
% \end{figure}

% A statistical analysis of the residuals between the model's predicted probabilities and the actual labels reveals,as shown in {\bf Figure \ref{fig_residual_analysis}},the mean residual of the predicted probabilities is -0.0959, close to zero, indicating no systematic overestimation or underestimation by the model. The skewness (-0.0851), also near zero, suggests the residual distribution is essentially symmetric. The positive kurtosis (1.5978) indicates that prediction errors are concentrated within a narrow range. Overall, the model's predictions are reliable, and the error distribution is reasonable.


% \begin{figure}[H]    % H表示强制固定在当前位置
% \centering
% \includegraphics[width=0.8\textwidth]{fig_residual_analysis.png}

% \caption{Residual Analysis} \label{fig_residual_analysis}
% \end{figure}

Next, robustness testing is conducted by adding Gaussian noise at varying levels (1\%–-20\%) to the input features to assess the model's stability when data contains minor errors. Noise is added proportionally to each feature's standard deviation to simulate measurement errors encountered during actual data acquisition.




\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.8\textwidth]{fig_robustness_noise.png}

\caption{Robustness Analysis: Noise Injection} \label{fig_robustness_noise}
\end{figure}
After adding 5\% noise to the data, the model's accuracy changed by only 0.43\%, and the ROC-AUC changed by 0.0060. After adding 10\% noise, the accuracy changed by +1.00\%. This indicates that the model possesses strong robustness against interference and can still reliably output stable results even when actual data contains minor measurement errors.


\subsection{Multi-Model Validation}

To validate the reliability of the Variance Decomposition Model for Problem 2, we employed multi-model cross-validation to enhance the model's feasibility and robustness. The selected validation models and their validation purposes are listed in the {\bf Table \ref{table_validation_models}} below.


\begin{figure}[H]
    \caption{Multi-Model Validation and Robustness Analysis}
    \centering
    % 调整宽度为0.45\textwidth，适配mcmthesis的页边距
    \begin{subfigure}{0.52\textwidth}
        
        \includegraphics[width=\textwidth]{fig_validation_summary.png}
        \caption{Validation Summary}
        \label{fig_validation_summary}  % 保持你原来的标签格式
    \end{subfigure}
    \quad  % 用quad控制固定间距（比hfill更稳定）
    \begin{subfigure}{0.42\textwidth}
        
        \includegraphics[width=\textwidth]{fig_robustness_analysis.png}
        \caption{Robustness Analysis}
        \label{fig_robustness_analysis}
    \end{subfigure}
\end{figure}

%% 三线表
\begin{table}[H]     % H表示强制固定在当前位置
\small
\centering           % 设置居中
\caption{Validation Models}  % 表标题
\label{table_validation_models}         % 设置表的引用标签
\begin{tabular}{lll}  % 2列, c表示居中对齐, l和r表示左右对齐
\toprule             % 画顶端横线
\textbf{Model}      & \textbf{Method Type}  & \textbf{Purpose of Verification}       \\
\midrule             % 画中间
Linear Regression   & Parameter Regression  & Quantify the weight contribution ratio \\
Logistic regression & Classification Model  & Analyze Factors Affecting Elimination  \\
BootstrapSimulation & Resampling Statistics & Constructing Confidence Intervals      \\
SHAP                & Game Theory           & Fair distribution of contributions    \\
\bottomrule         % 画底部横线
\end{tabular}
\vspace{2pt} % 稍微调整间距
\begin{flushleft} \small
\end{flushleft}
\end{table}



% \begin{figure}[H]    % H表示强制固定在当前位置

% \includegraphics[width=0.8\textwidth]{fig_validation_summary.png}
% \caption{Validation Summary} \label{fig_validation_summary}
% \end{figure}\begin{figure}[H]    % H表示强制固定在当前位置

% \includegraphics[width=0.7\textwidth]{fig_robustness_analysis.png}
% \caption{Robustness Analysis} \label{fig_robustness_analysis}
% \end{figure}


% 正文区直接插入


% \begin{figure}[H] 
% \caption{Uncertainty Propagation Analysis Results} 
% \centering
% \label{fig_propagation_analysis}
% \begin{subfigure}{0.5\textwidth}
% \includegraphics[width=0.9\textwidth]{fig_uncertainty_propagation (1).png}
% \end{subfigure}\begin{subfigure}{0.5\textwidth}
% \includegraphics[width=0.9\textwidth]{fig_sensitivity_analysis (1).png}
% \end{subfigure}\\
% \end{figure}

\vspace{-10pt}
As shown in the {\bf Figure \ref{fig_validation_summary}},all methods confirm that the percentage method yields higher audience weights than the ranking method, demonstrating consistency. Most methods indicate audience weights under the percentage method fall within the 50-90\% range, reflecting stronger audience dominance. Shapley values validate the rationality of weight allocation from a fair distribution perspective.



The model's performance stability was evaluated when introducing varying levels of noise into the input data. As shown in the {\bf Figure \ref{fig_robustness_analysis}}, performance remained largely consistent within the 0\%–20\% noise range without significant decline, indicating strong resistance to input noise interference. As noise levels increased, fluctuations in performance scores slightly widened, yet overall stability was maintained, validating the model's robustness.

\subsection{Spatial Exploration of KMDR Model Parameters}

In Problem 4,the distortion penalty coefficient $\alpha$ and Condorcet consensus reward coefficient $\beta$ are core parameters of the KMDR model. To investigate the model's response characteristics to variations in these two key parameters, we conducted a spatial exploration sensitivity analysis of the $\alpha-\beta$ parameters. We constructed an 8$\times$8 parameter grid with 0.1 increments within the interval [0.1, 0.8], yielding a total of 64 combinations. The elimination prediction accuracy was evaluated for each combination.

\begin{figure}[H]    % H表示强制固定在当前位置
\centering
\includegraphics[width=0.9\textwidth]{fig_sensitivity_heatmap.png}

\caption{KMDR Parameter Sensitivity Heatmap} \label{fig_kmdr_sensitivity}
\end{figure}

\vspace{-5pt}

The results of the parameter sensitivity heatmap analysis are shown in the figure. We find that the elimination prediction accuracy reaches its highest value of 23.4\% under the optimal parameter combination ($\alpha$=0.1, $\beta$=0.2), while the elimination accuracy under the current parameters is 20.0\% which falls within a reasonable range. The model exhibits sensitivity to variations in the β parameter. Excessively high $\beta$ values lead to over-rewarding of “moderate” performers. The $\alpha$ parameter demonstrates stable performance within the 0.2–0.6 range, indicating the robustness of the metric distortion penalty mechanism. This confirms the robustness and reliability of the KMDR model.


% \subsection{Sensitivity}

% In section 4.3, factor is introduced to estimate the parameters of the expected return
% of the market. Therefore, change the size of this parameter, that is, the capital market environment
% has changed. Below we analyze the sensitivity of this parameter. Gradual reduction of
% the parameter by $5\%$. The reason for considering only a decrease in r and not an increase in r
% is to reflect the worst-case scenario, i.e. a gradual decrease in capital market returns, and to see
% if our model is sensitive to the parameter.

% Therefore, Re-simulate the calculation results and obtain $3$ sets of curves as shown in
% Figure \ref{fig10}.

% 并排两个图(非子图)
% \begin{figure}[H]
% \begin{floatrow}
% \ffigbox[\FBwidth]
% {
% \includegraphics[width=0.48\textwidth]{fig10.jpg}
% }
% {\caption{Sensitivity analysis of $r_m$} \label{fig10}}
% \ffigbox[\FBwidth]
% {
% \includegraphics[width=0.48\textwidth]{fig11.jpg}
% }
% {\caption{Robustness analysis of $R_{value}$} \label{fig11}}
% \end{floatrow}
% \end{figure}

% The results show that as r decreases, the profit gained by the insurance company if it sells
% the insurance company at the same price tends to decrease. This makes sense because lower
% capital market interest rates result in lower profits for insurance companies. The trend of the
% curve obtained by sensitivity test is consistent with the actual situation.

% \subsection{Robustness}

% We verify the robustness of the model. Given the uncertainty of natural hazards, we may
% have errors in the calculation of Rvalue, which affects our grading of the risk indicators for each
% region. Randomly selecting some of the more than $3,000$ counties in the United States and
% deviating its Rvalue by $5\%$, and again grading these areas, the results obtained are shown in
% Figure \ref{fig11}.

% As can be seen from the figure, the grading of the original regions changes only slightly
% after randomly selecting regions to bias their measurements. This indicates that the small error
% in $R$ does not cause large changes in the model results and our model is stable.


\section{Strengths and Weaknesses}
{\bf \Large Strengths:}
\begin{itemize}
\vspace{-5pt}
\item Breaking through the limitations of a single model, we constructed a dual-reconstruction modeling strategy combining TAN and RF to reverse-engineer hidden audience voting data. The consistency rate between the two models reached 88.76\%.
\vspace{-5pt}
\item Establish a multi-level uncertainty quantification framework for problems 1 through 3, forming a complete uncertainty propagation chain that embodies systematic modeling thinking.
\vspace{-10pt}
\item The dynamic weighting design of the KMDR model achieves a dynamic equilibrium between professionalism and entertainment value, with a Condorcet efficiency of 98.79\%, significantly outperforming traditional fixed-weight mechanisms.
\vspace{-5pt}
\item All models underwent dual validation for both validity and robustness. For instance, the RF model in Question 1 achieved an average accuracy of 0.8524 through 10-fold stratified cross-validation, demonstrating strong generalization capabilities.
\end{itemize}

\vspace{-5pt}
{\bf \Large Weaknesses:}
\vspace{-5pt}
\begin{itemize}
\item The model simplification assumptions introduce bias and fail to incorporate practical influencing factors such as trending events, resulting in minor discrepancies between some estimated outcomes and the actual program scenarios.

\vspace{-5pt}
\item The sample and data dimensions have certain limitations, which may result in the model's insufficient adaptability to different scenarios, and its generalization capability requires improvement.

\vspace{-5pt}
\end{itemize}
% {\bf Robustness and Flexibility:} Our model demonstrates strong adaptability to various parameter
% variations through sensitivity analysis and robustness testing, and is able to provide
% reliable predictions under different scenarios.

% {\bf Comprehensive consideration:} Our model integrates social and economic factors, natural
% disaster risks, and provides a way to price insurance.


% {\bf Data Dependency:} Although the model has a high demand for data quality, it reflects our
% scientific attitude of pursuing accuracy and real-time performance. By working with data providers,
% we can continuously optimize the data collection and processing process.\cite{guo2024haier}


% {\bf Model Assumptions:} The model relies on certain assumptions that may not hold true in
\printbibliography[heading=bibintoc, title={References}]

\newpage
\addcontentsline{toc}{section}{Memorandum}
\begin{center}
{\Large \textbf{Memorandum}}   
\end{center}

\vspace{0.5cm}



\noindent {\bf To:Dancing with the Stars Production Team}

\noindent {\bf From: Team \#2615954}

\noindent {\bf Subject:DWTS Optimization Recommendations Based on Research}

\noindent {\bf Date: February 2, 2026}


It is our honor to write to your team to present our research findings on optimal fairness mechanisms for program voting and elimination processes. Below, we will outline the core elements of this study, focusing on the practical implementation of optimal fairness mechanisms to enhance the fairness, rationality, and entertainment value of your programs.


During our research, we analyzed data from the past 34 seasons of the program. By establishing a multi-level statistical model, we simulated the operational outcomes of various voting and elimination formats. Ultimately, we discovered that:
\begin{itemize}

    \item The percentage method places greater emphasis on audience voting than the ranking method;

    \item The judge intervention mechanism provides a chance to correct errors in disputed cases;

    \item The weighting of judging and audience voting is a crucial factor in the scoring mechanism.

\end{itemize}

Next, we will propose the following specific recommendations for the implementation and refinement of optimal fairness mechanisms:
\begin{itemize}
    \item   \textbf{Dynamic Weight Adjustment Mechanism:}
   
   Dynamically adjust the weighting of judges' and audience votes based on the characteristics of different stages of the program, adhering to the principle of optimal fairness mechanisms.

\item \textbf{Refine the Judge Rescue Mechanism:}
    
Continuously refine the rules governing the use of the judge rescue mechanism to address limitations in the voting process, thereby enhancing the fairness and acceptance of elimination outcomes.

\item \textbf{Standardize the voting process:}

Guide participants to vote rationally, mitigate unreasonable interference, ensure the authenticity and validity of voting results, and provide safeguards for implementing optimal fairness mechanisms.
\end{itemize}



We sincerely hope this research provides valuable insights for your program's operations, helping to refine its voting and elimination mechanisms.

Looking forward to seeing further success for Dancing with the Stars in the future!














\vspace{1.5cm}                    % 增加间距，把落款往下推

\hfill Sincerely yours,           % 右对齐

\hfill {\bf Team \#2615954}       % 右对齐

\newpage


% \begin{appendices}

% % \section{First Appendix}

% \textbf{\textcolor[rgb]{0.98,0.00,0.00}{Input Python source:}}
% \begin{lstlisting}[language=python]
% import numpy as np

% def fft(x):
%     n = len(x)
%     if n == 1:
%         return x
%     even = fft(x[0::2])
%     odd = fft(x[1::2])
%     T = [np.exp(-2j * np.pi * k / n) * odd[k] for k in range(n // 2)]
%     return [even[k] + T[k] for k in range(n // 2)] + [even[k] - T[k] for k in range(n // 2)]
% \end{lstlisting}

% \textbf{\textcolor[rgb]{0.98,0.00,0.00}{Input Matlab source:}}
% \lstinputlisting[language=Matlab]{./code/mcmthesis-matlab1.m}

% \end{appendices}


\AImatter

% \addcontentsline{toc}{section}{Report on Use of AI}

\begin{ReportAiUse}{9}

\bibitem{AI1}
OpenAI ChatGPT 5.2 

Query1: How Outstanding Winner Papers Present Narrative Structures ？

Output: This question hits the nail on the head, because what wins the Outstanding Winner award at the US Math Competition is never just the model—it's the story.

(1)The Essence of Outstanding Winner Papers: 

They Are Not Papers but "Technical Narrative Works"

(2)The "Standard Narrative Structure" of Outstanding Winner Papers (Deconstructed)
\begin{itemize}
\item Introduction: Not "What We Did," but "Why This Problem Deserves Serious Attention"
\item  Problem Deconstruction: Outstanding Winner Papers Always "Reframe the Problem Structure"
\item  Assumptions Section: The Assumptions in Outstanding Winner Papers Are "Restrained + Justified"
\item  Model Building: Models Are Not the Protagonists but "Plot Tools"
\item  Results Analysis: Outstanding Winner Papers Do Not Merely "Compute Results" but "Interpret Results"
\item  Sensitivity Analysis \& Robustness: This Is the "Source of Confidence" in Outstanding Winner Papers
\item  Conclusion: Not a Summary, but a "Return to the Original Problem"
\end{itemize}


% (1) Interactive Dashboards: Develop dashboards that allow users to interact with the data,
% such as filtering, zooming, and selecting specific data points to see detailed information.

% (2) Animated Visualizations: Use animations to show changes in data over time, helping
% to illustrate trends, cycles, or progressions.

% (3) Augmented Reality (AR) Visualizations: Implement AR to overlay data on real-world
% objects or environments, enhancing understanding of spatial and contextual relationships.

% (4) 3D Visualizations and Models: Employ 3D models to represent complex data structures,
% allowing exploration from different angles and perspectives.

% (5) Narrative Visualizations: Combine data visualization with storytelling, guiding viewers
% through a narrative that explains the significance of the data in a compelling way.

% (6) Real-time Data Streaming: Visualize data in real-time, updating visuals as new data
% comes in, ideal for monitoring and tracking systems.

% (7) Data Sonification: Convert data points into audio signals, making it possible to ``listen''
% to the data for patterns, trends, or anomalies.

% These ideas can help present data in more engaging, insightful, and accessible ways.

\bibitem{AI2}
OpenAI ChatGPT 5.2 

Query1: What is the format of the memo?

Output: Below is the format for the memo:


(1)Title Page

Title: Concise, descriptive of the issue or conclusion

Team Information: Team name, member names, student IDs, school

Date: Submission date

(2)Memo Recipient Information (To/From)

Similar to formal memo format for quick identification by “superiors/clients”:


To: [Recipient, e.g., Department Head/Client]  

From: [Your Team or Lead]  

Date: [Date]  

Subject: [Subject]  

(3)Executive Summary

Typically 1 paragraph to half a page, covering:

Brief research question

Primary methodology

Key findings/recommendations

Objective: Enable recipients to grasp results and recommendations without reading the full text

(4)Background

Concise description of research context and rationale

May include:
\begin{itemize}






\item Origin or context of the issue

\item Known conditions or constraints

\item Research objectives
\end{itemize}
(5)Methods

Clearly outline your research/analytical approach

Common format:

\begin{itemize}


\item Bulleted list of models and assumptions

\item Data sources and processing methods

\item Mathematical formulas or algorithms (may be included in appendices)

  
\end{itemize}




Note: Include only essential details; avoid excessive length

(6)Analysis/Discussion

Core analytical findings

Present using charts or tables

Content logic:

\begin{itemize}


\item Problem → Method → Result → Interpretation

\item Emphasize actionable conclusions

\item Avoid overly academic reasoning; prioritize clarity and directness

  
\end{itemize}




(7)Recommendations
\begin{itemize}


\item Propose actionable suggestions addressing the research question

\item Emphasize feasibility and concrete implementation

\item May be listed in short bullet points, each corresponding to an analysis result

  
\end{itemize}




(8)Formatting Recommendations

Font and Layout:

\begin{itemize}


\item Uniform font (e.g., Times New Roman or Arial)

\item Font size 11–12pt

\item Clear headings with distinct hierarchical levels

  
\end{itemize}





Figures and Tables:

\begin{itemize}
\item Complete titles and numbering

\item Clear in-text citations (referenced within main text)
\end{itemize}

Length:

\begin{itemize}

\item Typically 5–10 pages of main text + appendices

\item Avoid lengthy, thesis-style narratives
\end{itemize}


% construction industry, providing news, analysis, commentary, and data vital for professionals
% in the field. The publication's roots can be traced back to two separate publications from the
% late 19th century, eventually merging into what we know today as ENR in 1917. It has evolved
% to offer a wide range of content including top design firms, construction companies, projects
% in the U.S. and around the world, and industry trends affecting business management, design,
% construction methods, technology, and more. ENR is recognized for its comprehensive rankings
% of the largest construction and engineering firms, based on gross revenues, and for its ENR
% Award of Excellence, which honors individuals who have made significant contributions to the
% industry. Today, ENR is owned by BNP Media and continues to serve as an essential resource
% for contractors, project owners, engineers, architects, and other industry stakeholders with its
% print and digital magazine, website, newsletters, webinars, and events.

\bibitem{AI3}
OpenAI ChatGPT 5.2 

Query1: Where can we access real-time popularity data for contestants during the broadcast of Dancing with the Stars?

Output: To obtain real-time popularity data for the variety show Dancing with the Stars on Google platforms (i.e., search trends for the program or contestant names on Google), the most common method is to use Google Trends and its associated data interfaces/tools.

Google Trends reflects global search engine popularity changes for specific keywords (including program names or contestant names), serving as a gauge for audience engagement.



\bibitem{AI4}
OpenAI ChatGPT 5.2 

Query1: Please explain the definitions of fairness and entertainment value.

Output: Let’s break these two concepts down clearly and in context—especially since they often come up in competitions, media, or performance evaluation.

(1) Fairness

Definition: Fairness refers to the impartiality and equity in the way people, participants, or processes are treated. A process is fair if all participants have an equal opportunity to succeed and the rules are applied consistently.

(2) Entertainment Value

Definition: Entertainment value measures how enjoyable, engaging, or captivating a performance, product, or experience is to its audience. It reflects the subjective appeal and the ability to capture and maintain attention.






\bibitem{AI5}
OpenAI ChatGPT 5.2 

Query1: What does a high consistency coefficient between two predictive models signify?

Output:In data analysis, model evaluation, or metrology, an “excessively high consistency coefficient” (e.g., approaching or exceeding 90\%) warrants further interpretation. While it carries positive implications, it may also indicate underlying issues that require contextual assessment.


(1)Positive Implications: High Robustness of Models/Measurements

\begin{itemize}
\item High Model Consistency:

Despite differing algorithmic principles, the two models demonstrate strong convergence in interpreting the underlying patterns within the data.

\item High Reliability of Measurement Tools:

In fields like psychology and educational assessment (e.g., using Cronbach's alpha coefficient or inter-rater reliability), high consistency indicates that measurement tools (questionnaires, scoring criteria) are highly stable with minimal random error.

\item Clear intrinsic patterns and low noise in data:

High consistency sometimes indicates that the patterns to be predicted or classified within a dataset are very clear, with strong relationships between features and target variables and relatively little noise.
\end{itemize}

(2)Potential Risks and Issues Requiring Investigation

\begin{itemize}
\item Data leakage or feature overlap: 

This is the most critical risk to watch for. If two models use the same information that should not appear in predictions, it leads to a false high consistency.

\item Models Over-Reliance on Identical Strong Features: 

If one or two dominant features exist in the data and exhibit near-linear correlation with the outcome variable, any model will primarily base judgments on these features, causing predictions to converge.

\item Unreasonable Evaluation Methods or Data Partitioning: 

Evaluating consistency on the same training set or using cross-validation with data partitioning that fails to ensure independence may lead to overestimation of model generalization ability.


\end{itemize}

\end{ReportAiUse}

\end{document}

%% This work consists of these files mcmthesis.dtx,
%%                                   figures/ and
%%                                   code/,
%% and the derived files             mcmthesis.cls,
%%                                   mcmthesis-demo.tex,
%%                                   README,
%%                                   LICENSE,
%%                                   mcmthesis.pdf and
%%                                   mcmthesis-demo.pdf.
%%
%% End of file `mcmthesis-demo.tex'.
